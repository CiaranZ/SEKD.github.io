<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation - Wei Yang et al.">
  <meta name="description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta name="keywords" content="vision-language models, knowledge distillation, hierarchical VQA, multi-step reasoning, visual question answering, self-elicited knowledge">
  <meta name="author" content="Wei Yang, Yiran Zhu, Zilin Li, Xunjia Zhang, Hongtao Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="North China Electric Power University &amp; Donghua University">
  <meta property="og:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation">
  <meta property="og:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta property="og:url" content="https://ciaranz.github.io/SEKD.github.io/">
  <meta property="og:image" content="https://ciaranz.github.io/SEKD.github.io/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Self-Empowering VLMs - Research Preview">
  <meta property="article:published_time" content="2025-11-23T12:03:09.000Z">
  <meta property="article:author" content="Wei Yang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language models">
  <meta property="article:tag" content="knowledge distillation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- 没有就删掉这两行 -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation">
  <meta name="twitter:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta name="twitter:image" content="https://ciaranz.github.io/SEKD.github.io/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Self-Empowering VLMs - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation">
  <meta name="citation_author" content="Yang, Wei">
  <meta name="citation_author" content="Zhu, Yiran">
  <meta name="citation_author" content="Li, Zilin">
  <meta name="citation_author" content="Zhang, Xunjia">
  <meta name="citation_author" content="Wang, Hongtao">
  <meta name="citation_publication_date" content="2025-11-23">
  <meta name="citation_conference_title" content="arXiv preprint arXiv:2511.18415">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2511.18415.pdf">
  <meta name="citation_doi" content="10.48550/arXiv.2511.18415">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation - Wei Yang et al. | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation",
  "description": "Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.",
  "author": [
    {
      "@type": "Person",
      "name": "Wei Yang",
      "affiliation": {
        "@type": "Organization",
        "name": "Department of Computer, North China Electric Power University, Baoding, China"
      }
    },
    {
      "@type": "Person",
      "name": "Yiran Zhu",
      "affiliation": {
        "@type": "Organization",
        "name": "Department of Computer, North China Electric Power University, Baoding, China"
      }
    },
    {
      "@type": "Person",
      "name": "Zilin Li",
      "affiliation": {
        "@type": "Organization",
        "name": "School of Information and Intelligent Science, Donghua University, Shanghai, China"
      }
    },
    {
      "@type": "Person",
      "name": "Xunjia Zhang",
      "affiliation": {
        "@type": "Organization",
        "name": "Department of Computer, North China Electric Power University, Baoding, China"
      }
    },
    {
      "@type": "Person",
      "name": "Hongtao Wang",
      "affiliation": {
        "@type": "Organization",
        "name": "Department of Computer, North China Electric Power University, Baoding, China"
      }
    }
  ],
  "datePublished": "2025-11-23",
  "publisher": {
    "@type": "Organization",
    "name": "arXiv"
  },
  "url": "https://ciaranz.github.io/SEKD.github.io/",
  "image": "https://ciaranz.github.io/SEKD.github.io/static/images/social_preview.png",
  "keywords": [
    "vision-language models",
    "knowledge distillation",
    "hierarchical VQA",
    "multi-step reasoning",
    "self-elicited knowledge"
  ],
  "abstract": "Vision-language models (VLMs) contain rich taxonomic knowledge, but often fail on hierarchical visual question answering, where predictions must stay consistent along a coarse-to-fine path. We compare several inference paradigms and show that conditioning each step on previous answers is highly effective, yet the main bottleneck is maintaining cross-level state rather than recalling labels. To address this, we propose Self-Empowering Knowledge Distillation (SEKD), where a multi-step teacher VLM supervises a single-pass student using its own hard labels, soft distributions and hidden states, leading to large improvements in hierarchical consistency and strong gains on both in-domain and zero-shot taxonomies, as well as on challenging mathematical reasoning benchmarks, all without additional human annotations.",
  "citation": "Yang et al., Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation, arXiv:2511.18415, 2025.",
  "isAccessibleForFree": true,
  "license": "https://creativecommons.org/licenses/by/4.0/",
  "mainEntity": {
    "@type": "WebPage",
    "@id": "https://ciaranz.github.io/SEKD.github.io/"
  },
  "about": [
    {
      "@type": "Thing",
      "name": "Vision-language models"
    },
    {
      "@type": "Thing",
      "name": "Knowledge distillation"
    }
  ]
}
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "North China Electric Power University & Donghua University",
  "url": "https://YOUR_INSTITUTION_WEBSITE.com",
  "logo": "https://ciaranz.github.io/SEKD.github.io/static/images/favicon.ico",
  "sameAs": [
    "https://twitter.com/YOUR_TWITTER_HANDLE",
    "https://github.com/CiaranZ"
  ]
}
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: 换成你们自己的相关工作，或者把整个 more-works-container 删掉 -->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- Paper title -->
              <h1 class="title is-1 publication-title">
                Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Authors -->
                <span class="author-block">
                  <a href="#" target="_blank">Wei Yang</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Yiran Zhu</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Zilin Li</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Xunjia Zhang</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Hongtao Wang</a><sup>†</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- Affiliations / arXiv info -->
                <span class="author-block">
                  <sup>1</sup> Department of Computer, North China Electric Power University, Baoding, China<br>
                  <sup>2</sup> School of Information and Intelligent Science, Donghua University, Shanghai, China<br>
                  arXiv preprint arXiv:2511.18415 (2025)
                </span>
                <span class="eql-cntrb">
                  <small>
                    <br><sup>*</sup>Co-first authors &nbsp;&nbsp;
                    <sup>†</sup>Corresponding author
                  </small>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Paper PDF -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2511.18415.pdf" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Code -->
                  <span class="link-block">
                    <!-- TODO: 把链接改成你们代码仓库的地址 -->
                    <a href="https://github.com/YOUR_CODE_REPOSITORY" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- arXiv -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2511.18415" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                </div>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- 这里开始就是没有视频的版本 -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Vision-language models (VLMs) hold extensive taxonomic knowledge, yet they often struggle on hierarchical
                visual question answering, where predictions must remain consistent along a coarse-to-fine label path.
                We compare several inference paradigms and find that conditioning each step on previous answers is
                particularly effective, while the key limitation of current VLMs is maintaining cross-level state,
                rather than simply recalling labels. To overcome this, we propose Self-Empowering Knowledge Distillation
                (SEKD), in which a multi-step teacher VLM supervises a single-pass student through its own hard labels,
                soft distributions and hidden states. The resulting student model stays efficient while achieving large
                gains in hierarchical consistency on in-domain and zero-shot taxonomies, and also improves performance
                on challenging mathematical reasoning benchmarks, all without additional human annotations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/carousel1.jpg" alt="Overview of the Self-Empowering VLMs framework" loading="lazy"/>
              <h2 class="subtitle has-text-centered">
                Overview of our hierarchical VQA problem formulation and the Self-Empowering Knowledge Distillation framework.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/carousel2.jpg" alt="Visualization of hierarchical consistency in VLM reasoning" loading="lazy"/>
              <h2 class="subtitle has-text-centered">
                Visualization of hierarchical consistency across coarse-to-fine label paths before and after SEKD.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/carousel3.jpg" alt="Quantitative comparison of compact VLMs with and without SEKD" loading="lazy"/>
              <h2 class="subtitle has-text-centered">
                Quantitative improvements of compact VLMs with Self-Empowering VLMs on multiple hierarchical VQA benchmarks.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/carousel4.jpg" alt="Qualitative case studies of multi-step visual reasoning" loading="lazy"/>
              <h2 class="subtitle has-text-centered">
                Qualitative case studies showing more reliable multi-step reasoning traces after applying SEKD.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Paper poster -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Poster</h2>
          <!-- TODO: 换成你们真实的 poster PDF，没有的话删掉整个 section -->
          <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        </div>
      </div>
    </section>
    <!--End paper poster -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{yang2025selfempowering,
  title   = {Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation},
  author  = {Yang, Wei and Zhu, Yiran and Li, Zilin and Zhang, Xunjia and Wang, Hongtao},
  journal = {arXiv preprint arXiv:2511.18415},
  year    = {2025},
  doi     = {10.48550/arXiv.2511.18415},
  url     = {https://arxiv.org/abs/2511.18415}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the source code of this website, we just ask that you link back to
              this page in the footer. <br>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  <!-- End of Statcounter Code -->

</body>
</html>
