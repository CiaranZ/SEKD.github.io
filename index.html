<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation - Wei Yang et al.">
  <meta name="description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta name="keywords" content="vision-language models, knowledge distillation, hierarchical VQA, multi-step reasoning, visual question answering, self-elicited knowledge">
  <meta name="author" content="Wei Yang, Yiran Zhu, Zilin Li, Xunjia Zhang, Hongtao Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook（不再引用本地图片） -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="North China Electric Power University &amp; Donghua University">
  <meta property="og:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta property="og:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta property="og:url" content="https://ciaranz.github.io/SEKD.github.io/">
  <meta property="article:published_time" content="2025-11-23T12:03:09.000Z">
  <meta property="article:author" content="Wei Yang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language models">
  <meta property="article:tag" content="knowledge distillation">

  <!-- Twitter（不再引用本地图片） -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- 如无 Twitter，可删掉这两行 -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="twitter:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="citation_author" content="Yang, Wei">
  <meta name="citation_author" content="Zhu, Yiran">
  <meta name="citation_author" content="Li, Zilin">
  <meta name="citation_author" content="Zhang, Xunjia">
  <meta name="citation_author" content="Wang, Hongtao">
  <meta name="citation_publication_date" content="2025-11-23">
  <meta name="citation_conference_title" content="arXiv preprint arXiv:2511.18415">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2511.18415.pdf">
  <meta name="citation_doi" content="10.48550/arXiv.2511.18415">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation - Wei Yang et al. | Academic Research</title>
  
  <!-- 不再显式设置 favicon / apple-touch-icon，以避免额外图片引用 -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers（去掉 image 字段） -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation",
    "description": "Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.",
    "author": [
      {
        "@type": "Person",
        "name": "Wei Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Yiran Zhu",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Zilin Li",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Information and Intelligent Science, Donghua University, Shanghai, China"
        }
      },
      {
        "@type": "Person",
        "name": "Xunjia Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Hongtao Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      }
    ],
    "datePublished": "2025-11-23",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://ciaranz.github.io/SEKD.github.io/",
    "keywords": [
      "vision-language models",
      "knowledge distillation",
      "hierarchical VQA",
      "multi-step reasoning",
      "self-elicited knowledge"
    ],
    "abstract": "Vision-language models (VLMs) contain rich taxonomic knowledge, but often fail on hierarchical visual question answering, where predictions must stay consistent along a coarse-to-fine path. We compare several inference paradigms and show that conditioning each step on previous answers is highly effective, yet the main bottleneck is maintaining cross-level state rather than recalling labels. To address this, we propose Self-Empowering Knowledge Distillation (SEKD), where a multi-step teacher VLM supervises a single-pass student using its own hard labels, soft distributions and hidden states, leading to large improvements in hierarchical consistency and strong gains on both in-domain and zero-shot taxonomies, as well as on challenging mathematical reasoning benchmarks, all without additional human annotations.",
    "citation": "Yang et al., Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation, arXiv:2511.18415, 2025.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://ciaranz.github.io/SEKD.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-language models"
      },
      {
        "@type": "Thing",
        "name": "Knowledge distillation"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data（去掉 logo 图片） -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "North China Electric Power University & Donghua University",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/CiaranZ"
    ]
  }
  </script>

  <!-- 自定义样式：teaser 图片（摘要章节前） -->
  <style>
    .teaser-section {
      background-color: #ffffff; /* 和上面的 hero 一样白色，和下方摘要的浅灰区分开 */
    }
    .teaser-figure-wrapper {
      max-width: 900px;
      margin: 0 auto 1.5rem;
    }
    .teaser-figure {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 12px; /* 想要直角就把这一行删掉 */
    }
    .teaser-caption {
      text-align: center;
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
    }
  </style>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: 换成你们自己的相关工作，或者把整个 more-works-container 删掉 -->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <!-- 顶部 hero：标题、作者、按钮 -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- Paper title -->
              <h1 class="title is-1 publication-title">
                Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Authors -->
                <span class="author-block">
                  <a href="#" target="_blank">Wei Yang</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Yiran Zhu</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Zilin Li</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Xunjia Zhang</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Hongtao Wang</a><sup>†</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- Affiliations / arXiv info -->
                <span class="author-block">
                  <sup>1</sup> Department of Computer, North China Electric Power University, Baoding, China<br>
                  <sup>2</sup> School of Information and Intelligent Science, Donghua University, Shanghai, China<br>
                </span>
                <span class="eql-cntrb">
                  <small>
                    <br><sup>*</sup>Co-first authors &nbsp;&nbsp;
                    <sup>†</sup>Corresponding author
                  </small>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Paper PDF（外链） -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2511.18415.pdf" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Code：已改为你的代码仓库 -->
                  <span class="link-block">
                    <a href="https://github.com/CiaranZ/SEKD" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- arXiv -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2511.18415" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                </div>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser 图片：整个摘要章节前面，白色背景 -->
    <section class="section teaser-section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="teaser-figure-wrapper">
              <img
                src="static/images/7833a37b7e5ff0f37138ae35c7f558a3.png"
                alt="Overview of our hierarchical VQA formulation and SEKD framework"
                class="teaser-figure"
              >
            </div>
            <p class="teaser-caption">
              Overview of our hierarchical VQA formulation and Self-Elicited Knowledge Distillation (SEKD) framework.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Paper abstract（浅灰背景，只文字，没有图片） -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Vision-language models (VLMs) hold extensive taxonomic knowledge, yet they often struggle on hierarchical
                visual question answering, where predictions must remain consistent along a coarse-to-fine label path.
                We compare several inference paradigms and find that conditioning each step on previous answers is
                particularly effective, while the key limitation of current VLMs is maintaining cross-level state,
                rather than simply recalling labels. To overcome this, we propose Self-Empowering Knowledge Distillation
                (SEKD), in which a multi-step teacher VLM supervises a single-pass student through its own hard labels,
                soft distributions and hidden states. The resulting student model stays efficient while achieving large
                gains in hierarchical consistency on in-domain and zero-shot taxonomies, and also improves performance
                on challenging mathematical reasoning benchmarks, all without additional human annotations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{yang2025selfempowering,
  title   = {Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation},
  author  = {Yang, Wei and Zhu, Yiran and Li, Zilin and Zhang, Xunjia and Wang, Hongtao},
  journal = {arXiv preprint arXiv:2511.18415},
  year    = {2025},
  doi     = {10.48550/arXiv.2511.18415},
  url     = {https://arxiv.org/abs/2511.18415}
}</code></pre>
      </div>
    </section>
    <!--End BibTeX citation -->
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the source code of this website, we just ask that you link back to
              this page in the footer. <br>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  <!-- End of Statcounter Code -->

</body>
</html>
