<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation - Wei Yang et al.">
  <meta name="description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta name="keywords" content="vision-language models, knowledge distillation, hierarchical VQA, multi-step reasoning, visual question answering, self-elicited knowledge">
  <meta name="author" content="Wei Yang, Yiran Zhu, Zilin Li, Xunjia Zhang, Hongtao Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="North China Electric Power University &amp; Donghua University">
  <meta property="og:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta property="og:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta property="og:url" content="https://ciaranz.github.io/SEKD.github.io/">
  <meta property="article:published_time" content="2025-11-23T12:03:09.000Z">
  <meta property="article:author" content="Wei Yang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language models">
  <meta property="article:tag" content="knowledge distillation">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="twitter:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">

  <meta name="citation_title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="citation_author" content="Yang, Wei">
  <meta name="citation_author" content="Zhu, Yiran">
  <meta name="citation_author" content="Li, Zilin">
  <meta name="citation_author" content="Zhang, Xunjia">
  <meta name="citation_author" content="Wang, Hongtao">
  <meta name="citation_publication_date" content="2025-11-23">
  <meta name="citation_conference_title" content="arXiv preprint arXiv:2511.18415">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2511.18415.pdf">
  <meta name="citation_doi" content="10.48550/arXiv.2511.18415">
  
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation - Wei Yang et al. | Academic Research</title>
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation",
    "description": "Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.",
    "author": [
      {
        "@type": "Person",
        "name": "Wei Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Yiran Zhu",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Zilin Li",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Information and Intelligent Science, Donghua University, Shanghai, China"
        }
      },
      {
        "@type": "Person",
        "name": "Xunjia Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Hongtao Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      }
    ],
    "datePublished": "2025-11-23",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://ciaranz.github.io/SEKD.github.io/",
    "keywords": [
      "vision-language models",
      "knowledge distillation",
      "hierarchical VQA",
      "multi-step reasoning",
      "self-elicited knowledge"
    ],
    "abstract": "Vision-language models (VLMs) contain rich taxonomic knowledge, but often fail on hierarchical visual question answering, where predictions must stay consistent along a coarse-to-fine path. We compare several inference paradigms and show that conditioning each step on previous answers is highly effective, yet the main bottleneck is maintaining cross-level state rather than recalling labels. To address this, we propose Self-Empowering Knowledge Distillation (SEKD), where a multi-step teacher VLM supervises a single-pass student using its own hard labels, soft distributions and hidden states, leading to large improvements in hierarchical consistency and strong gains on both in-domain and zero-shot taxonomies, as well as on challenging mathematical reasoning benchmarks, all without additional human annotations.",
    "citation": "Yang et al., Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation, arXiv:2511.18415, 2025.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://ciaranz.github.io/SEKD.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-language models"
      },
      {
        "@type": "Thing",
        "name": "Knowledge distillation"
      }
    ]
  }
  </script>
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "North China Electric Power University & Donghua University",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/CiaranZ"
    ]
  }
  </script>

  <style>
    /* 顶部 hero 区块背景 */
    .hero-header {
      background: linear-gradient(180deg, #e0e7ff 0%, #f3f4ff 40%, #ffffff 100%) !important;
      position: relative;
      overflow: hidden;
    }
    .hero-header::before {
      content: "";
      position: absolute;
      inset: 0;
      background-image: radial-gradient(circle, rgba(148, 163, 184, 0.35) 1px, transparent 0);
      background-size: 42px 42px;
      opacity: 0.4;
      pointer-events: none;
    }
    .hero-header .hero-body {
      position: relative;
      z-index: 1;
    }

    /* 渐变标题文字，仅作用在 “Self-Empowering” */
    .gradient-title {
      background: linear-gradient(135deg, #4F46E5 0%, #9333EA 100%);
      -webkit-background-clip: text;
      color: transparent;
    }

    .teaser-section {
      background-color: #ffffff;
    }
    /* 让 teaser 这一块容器更宽一点 */
    .teaser-section .container {
      max-width: 1200px;
    }
    /* 图片容器放大一些 */
    .teaser-figure-wrapper {
      max-width: 1200px;
      margin: 0 auto 1.5rem;
    }
    .teaser-figure {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 12px;
    }
    .teaser-caption {
      text-align: center;
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
    }

    /* --- 新增：作者列表链接样式控制 --- */
    .publication-authors a {
      text-decoration: underline !important; /* 强制有链接的名字显示下划线 */
      color: inherit;
    }
    .publication-authors a:hover {
      color: #3273dc; /* 鼠标悬停变蓝 */
    }
    .publication-authors .no-link {
      text-decoration: none !important; /* 没有链接的名字无下划线 */
      cursor: default; /* 鼠标样式为普通指针 */
      color: inherit;
    }
  </style>
</head>
<body>

  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
    <section class="hero hero-header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">

              <h1 class="title is-1 publication-title" style="margin-bottom:0.5rem;">
                <span class="gradient-title">Self-Empowering</span> VLMs
              </h1>
              <h2 class="subtitle is-4" style="margin-top:0;">
                <strong>Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation</strong>
              </h2>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://miemie0922.github.io/weiyang.github.io/" target="_blank">Wei Yang</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <span class="no-link">Yiran Zhu</span><sup>*</sup>,
                </span>
                <span class="author-block">
                  <span class="no-link">Zilin Li</span>,
                </span>
                <span class="author-block">
                  <span class="no-link">Xunjia Zhang</span>,
                </span>
                <span class="author-block">
                  <span class="no-link">Hongtao Wang</span><sup>†</sup>
                </span>
              </div>
              
              <div class="is-size-5 publication-authors has-text-weight-normal has-text-grey-dark">
                  <span class="author-block">
                    <sup>1</sup> Department of Computer, North China Electric Power University, Baoding, China<br>
                    <sup>2</sup> School of Information and Intelligent Science, Donghua University, Shanghai, China<br>
                  </span>
                <span class="eql-cntrb">
                  <small>
                    <br><sup>*</sup>Co-first authors &nbsp;&nbsp;
                    <sup>†</sup>Corresponding author
                  </small>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2511.18415.pdf" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/CiaranZ/SEKD" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2511.18415" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                </div>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section teaser-section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="teaser-figure-wrapper">
              <img
                src="static/images/7833a37b7e5ff0f37138ae35c7f558a3.png"
                alt="Overview of our hierarchical VQA formulation and SEKD framework"
                class="teaser-figure"
              >
            </div>
            <p class="teaser-caption">
              Overview of our hierarchical VQA setup and Self-Elicited Knowledge Distillation (SEKD): a taxonomic query is flattened into a sequence of level-wise VQA questions along the taxonomy; the original VLM answers each level independently and often yields an inconsistent path, while SEKD lets the model solve easy levels first and reuse them to guide later ones, producing a post-SEKD VLM that follows a coherent coarse-to-fine hierarchy.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks,
                where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels.
                We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned
                on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main
                limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic
                knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires
                no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by
                exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills
                these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher.
                It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen
                taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision
                is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route
                to imbue compact VLMs with dependency-aware multi-step reasoning.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{yang2025selfempowering,
  title   = {Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation},
  author  = {Yang, Wei and Zhu, Yiran and Li, Zilin and Zhang, Xunjia and Wang, Hongtao},
  journal = {arXiv preprint arXiv:2511.18415},
  year    = {2025},
  doi     = {10.48550/arXiv.2511.18415},
  url     = {https://arxiv.org/abs/2511.18415}
}</code></pre>
      </div>
    </section>
    </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the source code of this website, we just ask that you link back to
              this page in the footer. <br>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
