<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation - Wei Yang et al.">
  <meta name="description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta name="keywords" content="vision-language models, knowledge distillation, hierarchical VQA, multi-step reasoning, visual question answering, self-elicited knowledge">
  <meta name="author" content="Wei Yang, Yiran Zhu, Zilin Li, Xunjia Zhang, Hongtao Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="North China Electric Power University &amp; Donghua University">
  <meta property="og:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta property="og:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">
  <meta property="og:url" content="https://ciaranz.github.io/SEKD.github.io/">
  <meta property="article:published_time" content="2025-11-23T12:03:09.000Z">
  <meta property="article:author" content="Wei Yang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language models">
  <meta property="article:tag" content="knowledge distillation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="twitter:description" content="Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation">
  <meta name="citation_author" content="Yang, Wei">
  <meta name="citation_author" content="Zhu, Yiran">
  <meta name="citation_author" content="Li, Zilin">
  <meta name="citation_author" content="Zhang, Xunjia">
  <meta name="citation_author" content="Wang, Hongtao">
  <meta name="citation_publication_date" content="2025-11-23">
  <meta name="citation_conference_title" content="arXiv preprint arXiv:2511.18415">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2511.18415.pdf">
  <meta name="citation_doi" content="10.48550/arXiv.2511.18415">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation - Wei Yang et al. | Academic Research</title>
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation",
    "description": "Self-Empowering VLMs uses Self-Empowering Knowledge Distillation to transfer hierarchical multi-step reasoning from a teacher VLM to a compact student model.",
    "author": [
      {
        "@type": "Person",
        "name": "Wei Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Yiran Zhu",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Zilin Li",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Information and Intelligent Science, Donghua University, Shanghai, China"
        }
      },
      {
        "@type": "Person",
        "name": "Xunjia Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      },
      {
        "@type": "Person",
        "name": "Hongtao Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Department of Computer, North China Electric Power University, Baoding, China"
        }
      }
    ],
    "datePublished": "2025-11-23",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://ciaranz.github.io/SEKD.github.io/",
    "keywords": [
      "vision-language models",
      "knowledge distillation",
      "hierarchical VQA",
      "multi-step reasoning",
      "self-elicited knowledge"
    ],
    "abstract": "Vision-language models (VLMs) contain rich taxonomic knowledge, but often fail on hierarchical visual question answering, where predictions must stay consistent along a coarse-to-fine path. We compare several inference paradigms and show that conditioning each step on previous answers is highly effective, yet the main bottleneck is maintaining cross-level state rather than recalling labels. To address this, we propose Self-Empowering Knowledge Distillation (SEKD), where a multi-step teacher VLM supervises a single-pass student using its own hard labels, soft distributions and hidden states, leading to large improvements in hierarchical consistency and strong gains on both in-domain and zero-shot taxonomies, as well as on challenging mathematical reasoning benchmarks, all without additional human annotations.",
    "citation": "Yang et al., Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation, arXiv:2511.18415, 2025.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://ciaranz.github.io/SEKD.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-language models"
      },
      {
        "@type": "Thing",
        "name": "Knowledge distillation"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "North China Electric Power University & Donghua University",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/CiaranZ"
    ]
  }
  </script>

  <!-- Custom Styles -->
  <style>
    /* 顶部 hero 区块背景 */
    .hero-header {
      background: linear-gradient(180deg, #e0e7ff 0%, #f3f4ff 40%, #ffffff 100%) !important;
      position: relative;
      overflow: hidden;
    }
    .hero-header::before {
      content: "";
      position: absolute;
      inset: 0;
      background-image: radial-gradient(circle, rgba(148, 163, 184, 0.35) 1px, transparent 0);
      background-size: 42px 42px;
      opacity: 0.4;
      pointer-events: none;
    }
    .hero-header .hero-body {
      position: relative;
      z-index: 1;
    }

    /* 渐变标题文字，仅作用在 “Self-Empowering” */
    .gradient-title {
      background: linear-gradient(135deg, #4F46E5 0%, #9333EA 100%);
      -webkit-background-clip: text;
      color: transparent;
    }

    .teaser-section {
      background-color: #ffffff;
    }
    /* 让 teaser 这一块容器更宽一点 */
    .teaser-section .container {
      max-width: 1200px;
    }
    /* 图片容器放大一些 */
    .teaser-figure-wrapper {
      max-width: 1200px;
      margin: 0 auto 1.5rem;
    }
    .teaser-figure {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 12px;
    }
    .teaser-caption {
      text-align: center;
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
    }
  </style>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
    <!-- 顶部 hero：标题、作者、按钮 -->
    <section class="hero hero-header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">

              <!-- 大标题 -->
              <h1 class="title is-1 publication-title" style="margin-bottom:0.5rem;">
                <span class="gradient-title">Self-Empowering</span> VLMs
              </h1>
              <!-- 副标题 -->
              <h2 class="subtitle is-4" style="margin-top:0;">
                <strong>Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation</strong>
              </h2>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="#" target="_blank">Wei Yang</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Yiran Zhu</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Zilin Li</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Xunjia Zhang</a>,
                </span>
                <span class="author-block">
                  <a href="#" target="_blank">Hongtao Wang</a><sup>†</sup>
                </span>
              </div>
              <div class="is-size-5 publication-authors has-text-weight-normal has-text-grey-dark">
                  <span class="author-block">
                    <sup>1</sup> Department of Computer, North China Electric Power University, Baoding, China<br>
                    <sup>2</sup> School of Information and Intelligent Science, Donghua University, Shanghai, China<br>
                  </span>
                <span class="eql-cntrb">
                  <small>
                    <br><sup>*</sup>Co-first authors &nbsp;&nbsp;
                    <sup>†</sup>Corresponding author
                  </small>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2511.18415.pdf" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/CiaranZ/SEKD" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2511.18415" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                </div>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3 has-text-centered">Introduction</h2>
            <div class="content has-text-justified">
              <p>
                Vision-language models (VLMs) are good at open-ended recognition and QA, but they often break down on hierarchical visual QA, where predictions must form a coherent coarse-to-fine path in a taxonomy. In this work, we:
              </p>
              <ul>
                <li>
                  Formulate hierarchical VQA as predicting a full taxonomy path and systematically compare different inference paradigms (single-pass vs. multi-step, conditioned vs. unconditioned).
                </li>
                <li>
                  Diagnose that the main bottleneck is not missing taxonomic labels, but the inability of standard VLMs to maintain a consistent cross-level state across the hierarchy.
                </li>
                <li>
                  Propose <strong>Self-Elicited Knowledge Distillation (SEKD)</strong>, where a VLM first acts as its own multi-step teacher and then trains a single-pass student using only self-generated signals, without any human labels or external tools.
                </li>
                <li>
                  Show that SEKD greatly improves hierarchical path consistency and transfers to unseen taxonomies and challenging mathematical reasoning benchmarks, while keeping single-pass inference cost.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="overview" class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths has-text-centered">
    
            <!-- 标题 -->
            <h2 class="title is-3">Overview</h2>
    
            <!-- 大图 -->
            <div class="teaser-figure-wrapper" style="margin-top:1.5rem; margin-bottom:1.5rem;">
              <img
                src="static/images/7833a37b7e5ff0f37138ae35c7f558a3.png"
                alt="Overview of our hierarchical VQA setting and SEKD framework"
                class="teaser-figure"
              >
            </div>
    
            <!-- 下面的文字区：介绍实验块 + 效果 -->
            <div class="content has-text-justified" style="text-align:left; margin:0 auto; max-width: 52rem;">
              <p>
                We evaluate SEKD on three regimes and summarize the overall behaviour in this figure. In hierarchical visual QA, a query is decomposed into a sequence of level-wise questions along a taxonomy, and predictions are required to form a single consistent path. Standard VLMs often make locally plausible but globally inconsistent choices, jumping across branches, whereas SEKD enforces a more stable cross-level state.
              </p>
              <ul>
                <li>
                  <strong>In-domain hierarchical VQA.</strong>
                  Across our hierarchical VQA benchmarks, SEKD improves hierarchical path consistency (HCA) by up to
                  <strong>+29.50</strong> points over strong single-pass VLM baselines, while keeping single-pass inference cost.
                </li>
                <li>
                  <strong>Zero-shot new taxonomies.</strong>
                  A student trained via SEKD on one taxonomy transfers to an unseen taxonomy, raising zero-shot HCA from
                  <strong>4.15%</strong> to <strong>42.26%</strong> without any additional labels.
                </li>
                <li>
                  <strong>Mathematical reasoning.</strong>
                  Applying the same self-elicited distillation strategy to challenging math reasoning benchmarks yields consistent
                  accuracy gains over the base VLM, showing that SEKD is not limited to taxonomies.
                </li>
              </ul>
            </div>
    
          </div>
        </div>
      </div>
    </section>


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{yang2025selfempowering,
  title   = {Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Empowering Knowledge Distillation},
  author  = {Yang, Wei and Zhu, Yiran and Li, Zilin and Zhang, Xunjia and Wang, Hongtao},
  journal = {arXiv preprint arXiv:2511.18415},
  year    = {2025},
  doi     = {10.48550/arXiv.2511.18415},
  url     = {https://arxiv.org/abs/2511.18415}
}</code></pre>
      </div>
    </section>
    <!--End BibTeX citation -->
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the source code of this website, we just ask that you link back to
              this page in the footer. <br>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
